# Configuration version (required)
version: 1.2.8

# This setting caches the config file for faster loading across app lifecycle
cache: true

endpoints:
  custom:
    - name: "OpenRouter"
      apiKey: "${OPENROUTER_API_KEY}"
      baseURL: "https://openrouter.ai/api/v1"
      models:
        default: ["anthropic/claude-3.5-sonnet", "google/gemini-pro-1.5", "openai/gpt-4o"]
        fetch: true  # Automatically fetch available models from OpenRouter
      titleConvo: true
      titleModel: "anthropic/claude-3.5-sonnet"
      summarize: false
      summaryModel: "anthropic/claude-3.5-sonnet"
      forcePrompt: false
      modelDisplayLabel: "OpenRouter"

    - name: "Local AI"
      apiKey: "${LOCAL_AI_API_KEY}"
      baseURL: "${LOCAL_AI_BASE_URL}"
      models:
        default: ["llama3-8b", "mistral-7b"] # Replace with your actual model names
        fetch: true  # Fetch models from your local server
      titleConvo: true
      titleModel: "current_model"
      summarize: false
      forcePrompt: false
      modelDisplayLabel: "Local Server"

mcpServers:
  homeassistant:
    type: streamable-http
    url: "${HOME_ASSISTANT_URL}/api/mcp"
    headers:
      Authorization: "Bearer ${HOME_ASSISTANT_ACCESS_TOKEN}"
    chatMenu: true
